# DQNTraderを作成するに当たって学習もしくは考えた事
そもそもLSTMのHidden_steateを保持するコードに以前のLTSMnetを作っていなかった。要修正検証   
ステートフルLSTMにしなければ全層結合と予測精度はほぼ同じになってしまうため、LSTMの意味がない。LSTMTraderをどうにかする。このDQNTraderではLSTMNetの内容も一部使っているため関係あると言えばある。  
ステートフルLSTMにする場合バッジ内のデータをシャッフルする必要はない。DQNでは実際にゲームを一通りやって学習するため関係ないが、データを一気に用意しておいて学習させる普通の学習と同じような奴ではお手本通りbatch内のデータをシャッフルしてしまっていたので直す。   
ただ、 Bootstrapped Sequential Updates（一通りhidden_stateをリセットせずに学習する）方法とBootstrapped Random Updates(ランダムな点から開始して開始するごとにhidden stateがリセットされる)手法の２つがあることを考慮する。   
報酬がまちまちでガバガバ。-1~1でクリッピングする。   
hidden_stateはLSTMが各タイムステップで生成する内部の状態であり、パラメーターの更新で更新されるパラメーターではない。あくまでも中間的な結果ということ。   
こう考えると、hidden_stateは短期記憶？と言える。これがリセットされるステートレスLSTMでは時系列の短期記憶がforwardを実行するごとにリセットされるため、いわば完璧に学習した記憶（パラメータの更新で獲得した記憶）は思い出せても最近の事とこれらが時系列で思い出せない老人状態になっている。   
バッジ内でのデータの相互影響がなくなる。   
   
24/12/27のDDQNではnext_stateが+1フレーム後で株価の予測ではまったく意味がない。
そこでnext_stateが現在のnフレームから (+1 ~ +x)frame間のデータを使う必要がある。これは行動の結果がすぐに表れない環境での強化学習では必須だと思う。   
複数フレーム分の報酬を累積して１つの値として使うことで、行動と結果の関係を強化。   
Double DQNの利点であるQ値の過大評価に関して、株価予測などのトレーニングデータでアクション後の値動きが確定している場合、そもそもQ値の過大評価が起こりえない   
Double DQNは未来が確定していない状況で今のアクションのQ値を評価するためにターゲットネットを利用して、+1Frame後に最適なQ値を計算してこれと現在のアクションのQ値の違いを計算し評価する。   

(next_state)の形をnフレーム間に適切なactionに変える


現在はポジション設定に対して報酬を設定していないが、これに適切な報酬を設定することで学習が進むことがある。   
[ChatGPTの解説](Positions_Reward.md)

pytorch   
tensor.gather(dim=n, index=indices)   
はdim=0の場合tensorの列のindexをindicesにしたがって集めていく   
dim=1の場合行のvalueをindicesにしたがって集めていくということ   
[参考](https://www.osumoi-stdio.com/pyarticle/book/18/17)   

